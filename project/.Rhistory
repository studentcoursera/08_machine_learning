text(63, 100, paste("mu = ", mu))
text(65, 100, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(65, 240, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(65, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=4)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2,1,3,1)
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=4)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=4)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(1.75, 150, paste("mu = ", mu))
text(2, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=4)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(1.75, 150, paste("mu = ", mu))
text(2, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(2, 0, paste("mu = ", mu))
text(2, 1, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(65, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(0, 1.75, paste("mu = ", mu))
text(1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(0, 1.75, paste("mu = ", mu))
text(0, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.0002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-2, 2, step = 0.02))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-1.54, 0.95, step = 0.02))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 4)))
}
manipulate(myHist(mu), mu = slider(-1.54, 0.95, step = 0.002))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=5)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 4)))
}
manipulate(myHist(mu), mu = slider(-1.54, 0.95, step = 0.0001))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=8)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(-1.54, 0.95, step = 0.0001))
library(manipulate)
myHist <- function(mu){
hist(x,col="blue",breaks=5)
lines(c(mu, mu), c(0, 4),col="red",lwd=8)
mse <- mean((x - mu)^2)
text(-1, 1.75, paste("mu = ", mu))
text(-1, 2, paste("MSE = ", round(mse, 5)))
}
manipulate(myHist(mu), mu = slider(-1.54, 0.95, step = 0.0001))
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
hist(x,col="blue",breaks=100)
meanChild <- mean(x)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
hist(x,col="blue",breaks=100)
meanChild <- mean(x)
lines(rep(meanChild,100),seq(0,15,length=10),col="red",lwd=5)
hist(x,col="blue",breaks=10)
meanChild <- mean(x)
lines(rep(meanChild,10),seq(0,15,length=10),col="red",lwd=5)
meanChild
install.packages("caret")
packageVersion("caret")
library("caret")
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
install.packages("kernlab")
library("caret")
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.sedd(32343)
modelFit <- train(type ~., data=training, method="glm")
modelFit
set.seed(32343)
modelFit <- train(type ~., data=training, method="glm")
modelFit
require(e1071)
set.seed(32343)
modelFit <- train(type ~., data=training, method="glm")
modelFit
requireNamespaceQuietStop("e1071")
install.packages("e1071")
require(e1071)
set.seed(32343)
modelFit <- train(type ~., data=training, method="glm")
modelFit
warnings()
set.seed(32343)
modelFit <- train(type ~., data=training, method="glm")
modelFit
modelFit <- train(type ~ ., data=training, method="glm")
modelFit$finalModel
predictions <- predict(modelFit, newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,list=T,returnTrain=T)
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,list=T,returnTrain=F)
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
folds <- createResample(y=spam$type,times=10,list=T)
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,horizon=10)
name(folds)
fold$train[[1]]
folds$test[[1]]
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,horizon=10)
names(folds)
fold$train[[1]]
folds$test[[1]]
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,horizon=10)
names(folds)
folds$train[[1]]
folds$test[[1]]
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
modelFit <- train(type ~ ., data=training, method="glm")
args(train.default)
args(trainControl)
set.seed(1235)
modelFit2 <- train(type ~ ., data = training, method="glm")
modelFit2
set.seed(1235)
modelFit2 <- train(type ~ ., data = training, method="glm")
modelFit2
packageVersion("caret")
install.packages("caret")
.libPaths()
require(devtools)
install_version("caret", version = "6.0.47", repos = "http://cran.us.r-project.org")
install.packages(devtools)
install.packages("devtools")
require(devtools)
install_version("caret", version = "6.0.47", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("caret", version = "6.0-47", repos = "http://cran.us.r-project.org")
require("caret")
install.packages("https://cran.r-project.org/src/contrib/Archive/caret/caret_6.0-47.tar.gz", repos=NULL, type="source")
require("caret")
packageVersion("caret")
pdf_document:
;
---
title: 'Machine Learning: Prediction (Project)'
author: "Ambika J, as on 12-Oct-2015"
output:
html_document:
fig_caption: yes
toc: yes
---
<style media="screen" type="text/css">
.layer1 {
margin-left: auto;
margin-right: auto;
width: 95%;
background-color: #b0e0e6;
}
.heading {
margin: 1px;
color: #000;
padding: 3px 10px;
cursor: pointer; cursor: hand;
position: relative;
background-color:#ffffcc;
}
.content {
background-color:#ffffcc;
}
p { padding: 5px 0; }
</style>
```{r function, echo=FALSE, results='hide', message=FALSE}
##=========================================##
## load_package/s:
## get the list of packages.
## parameter: packages
##
## For each package:
##         checks if package exists
##   if it doesnt, then, installs that package.
##   parameter: package_name
##=========================================##
load_packages <- function(pkgs) {
for (i in 1:length(pkgs)) load_package(pkgs[i])
}
load_package <- function(pkg) {
if (!isPackageInstalled(pkg)) install.packages(pkg,repos = "http://cran.us.r-project.org")
require(pkg, character.only = TRUE)
}
if (!is.element("R.utils", installed.packages())) install.packages("R.utils",repos = "http://cran.us.r-project.org")
require("R.utils")
load_packages(c("caret","randomForest","stargazer","datasets","e1071","UsingR","plyr","gridExtra","knitr"))
```
```{r pkgs, echo=FALSE, results='hide', eval=FALSE}
## List of packages required
library(caret)
library(randomForest)
library(stargazer)
library(e1071)
library(datasets)
library(UsingR)
library(plyr)
library(gridExtra)
library(knitr)
# Hlavac, Marek (2015). stargazer: Well-Formatted Regression and Summary Statistics Tables.
# R package version 5.2. http://CRAN.R-project.org/package=stargazer
```
```{r, echo=FALSE}
#rm(list=ls()) ## clear the working space
```
### Loading data
```{r filedownload}
## Loading the data
setwd("~/gitdir/coursera/studentcoursera/machine_learning/project")
if (!file.exists("data"))  dir.create("data")
## File download and unzip
if(!file.exists("data/pml_training.rds")) {
if(!file.exists("data/pml-training.csv"))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "./data/pml-training.csv", method = "curl")
}
if(!file.exists("data/pml_testing.rds")) {
if(!file.exists("data/pml-testing.csv"))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "./data/pml-testing.csv", method = "curl")
}
```
```{r origData,echo=FALSE,results=FALSE}
## ORIGINAL data load
## this can be DELETED from the final project; this is only for verification, reference;
#sfile="http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv"
#dfile="~/Coursera/CourseraCourses/JohnsHopkins-DataAnalysis-courseraCourse/08-MachineLearning/08-Project/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv"
#    if(!file.exists(dfile)) download.file(sfile, destfile = dfile, method = "curl")
#if (!"pmldata" %in% ls()) pmldata <- read.csv(dfile,na.strings=c(""))
### data clean-up; assuming roll_belt.1 is pitch_belt
#names(pmldata)[names(pmldata) == "skewness_roll_belt.1"] <- "skewness_pitch_belt"
#names(pmldata)[names(pmldata) == "var_total_accel_belt"] <- "var_accel_belt"
#names(pmldata) <- sub("_picth_","_pitch_",names(pmldata))
```
### Preprocessing the basic data
```{r loadData}
if (!"pmltraining" %in% ls())
pmltraining <- read.csv("data/pml-training.csv",na.strings=c(""))
if (!"pmltesting" %in% ls())
pmltesting  <- read.csv("data/pml-testing.csv",na.strings=c(""))
## Data cleaning
## assuming that this is misspelt for "skewness_pitch_belt" instead of "skewness_roll_belt.1"
names(pmltraining)[names(pmltraining) == "skewness_roll_belt.1"] <- "skewness_pitch_belt"
names(pmltraining)[names(pmltraining) == "var_total_accel_belt"] <- "var_accel_belt"
names(pmltraining) <- sub("_picth_","_pitch_",names(pmltraining))
names(pmltesting)[names(pmltesting) == "skewness_roll_belt.1"] <- "skewness_pitch_belt"
names(pmltesting)[names(pmltesting) == "var_total_accel_belt"] <- "var_accel_belt"
names(pmltesting) <- sub("_picth_","_pitch_",names(pmltesting))
```
### Extract the required fields
```{r reqFields}
## Here, we extract, first 7 and last 1 fields (8 fields), then, 16 fields of roll/pitch/yaw/total_accel for arm/belt/dumbbell/forearm, then, 36 fields of gyros/accel/magnet for arm/belt/dumbbell/forearm for x/y/z. [8 + 16 + 36 = 60 fields]
#pmltrain <- subset(pmltraining,c(1:7,grep("^roll_|^pitch_|^yaw_",names(pmltraining)),grep("^gyros_|^accel_|^magnet_|^total_accel",names(pmltraining)),grep("classe",names(pmltraining)))
### I think, only username, num_window, accelerator
pmltrain <- subset(pmltraining,select=c(2,7,grep("^accel_|^total_accel",names(pmltraining)),grep("classe",names(pmltraining))))
```
### Data Explorartory
```{r dataExploratory,  cache=TRUE, results='asis'}
#summary(pmltrain)
ddply(pmltrain, .(user_name), summarise, max=max(accel_belt_x), min=min(accel_belt_x),
avg=mean(accel_belt_x), var=var(accel_belt_x), sd=sd(accel_belt_x))
ddply(pmltrain, .(classe), summarise, max=max(accel_belt_x), min=min(accel_belt_x),
avg=mean(accel_belt_x), var=var(accel_belt_x), sd=sd(accel_belt_x))
tmpAgg <- aggregate(pmltrain[,c(-1,-2,-19)],
by=list(pmltrain$user_name),
FUN = function(xx) c(
avg=round(mean(xx),2), var=round(var(xx),2), stddev=round(sd(xx),2),
min=min(xx), max=max(xx), kurtosis=round(kurtosis(xx),2),
skewness=round(skewness(xx),2), amplitude=round(max(xx) - min(xx),2)
))
#library(xtable)
#xt <- xtable(tmpAgg)
#print(xt, type="html")
#stargazer(head(pmltrain, n=50), summary = TRUE, type = "html")
stargazer(pmltrain, type = "html", header = TRUE,
title = "Table 1 Summary of all the variables",
summary = TRUE, omit.summary.stat = "n", out = "table1.html",
rownames = TRUE , digits = 1,
notes = "1. Numbers dispayed are in thousand", style = "all")
### Plots to check how the variables are
```
### Data partition: Training and testing data
```{r}
### Creating training and testing data set: 60% and 40%
set.seed(3433)
trainIndex <- createDataPartition(pmltrain$classe,p=0.6)[[1]]
training = pmltrain[trainIndex,]
testing = pmltrain[-trainIndex,]
## Splitting testing into two halves: test and val
testIndex <- createDataPartition(testing$classe,p=0.5)[[1]]
testing_test = pmltrain[trainIndex,]
testing_val = pmltrain[-trainIndex,]
#rm(list = grep("pml",ls(), value=T))
```
### Checks
```{r checks}
### NA check
w1 <- sapply(training, function(x) all(is.na(x)))
w2 <- sapply(testing,  function(x) all(is.na(x)))
w3 <- sapply(training, function(x) any(is.na(x)))
w4 <- sapply(testing,  function(x) any(is.na(x)))
paste(length(w1[w1]),length(w2[w2]),length(w3[w3]),length(w4[w4]),sep=",")
#[1] "0,0,0,0"
### 0s check
w1 <- sapply(training, function(x) all(x == 0))
w2 <- sapply(testing,  function(x) all(x == 0))
w3 <- sapply(training, function(x) any(x == 0))
w4 <- sapply(testing,  function(x) any(x == 0))
paste(length(w1[w1]),length(w2[w2]),length(w3[w3]),length(w4[w4]),sep=",")
#[1] "0,0,15,15"
## Now, for these 48 fields and 15 fields, check it.
pmltrain0sfields <- training[,names(w3[w3])]
pmltest0sfields  <-  testing[,names(w4[w4])]
```
### Preprocess the predict data
```{r preprocessPredict}
### Variability check
nzv <- nearZeroVar(training, saveMetrics=T)
## filter out fields which are non-zero variability and zero variables.
training <- training[!nzv$nzv & !nzv$zeroVar,]
testing  <-  testing[!nzv$nzv & !nzv$zeroVar,]
### PCA check
```
### Predictors
```{r predict, cache=TRUE}
## predictors without PCA
if (!"modelFit1" %in% ls())
modelFit1 <- train(classe ~., data=training, method="rf", ntree=100)
pred <- predict(modelFit1, testing)
#qplot(classe, pred, data=testing)
confusionMatrix(testing$classe,predict(modelFit1,testing))
modFitAll <- train(classe ~., data=training, method="glm")
pred <- predict(modFitAll, testing)
#qplot(classe, pred, data=testing)
## predictors with PCA
```
table(training$classe)
qplot(accel_belt_x,accel_belt_y,colour=classe,data=training)
qplot(accel_belt_x,accel_belt_z,colour=classe,data=training)
qplot(accel_belt_x,accel_belt_z,colour=classe,data=testing)
qplot(accel_belt_x,accel_arm_x,colour=classe,data=testing)
qplot(accel_belt_x,accel_dumbbell_x,colour=classe,data=testing)
?Qplot
?qplot
qplot(accel_belt_x,accel_forearm_x,colour=classe,data=testing)
qplot(accel_arm_x,accel_forearm_x,colour=classe,data=testing)
qplot(log(accel_arm_x),log(accel_forearm_x),colour=classe,data=testing)
qplot(accel_arm_x,accel_forearm_x,colour=classe,data=testing)
qplot(accel_arm_x,accel_forearm_x,colour=classe,data=training)
qplot(accel_arm_x,accel_forearm_x,colour=classe,data=pmltraining)
qplot(accel_arm_x,gyros_arm_x,colour=classe,data=pmltraining)
qplot(accel_arm_x,magnet_arm_x,colour=classe,data=pmltraining)
qplot(accel_arm_y,magnet_arm_y,colour=classe,data=pmltraining)
qplot(accel_arm_z,magnet_arm_z,colour=classe,data=pmltraining)
qplot(pitch_arm,roll_arm,colour=classe,data=pmltraining)
qplot(pitch_arm,yaw_arm,colour=classe,data=pmltraining)
table(training$total_accel_arm)
table(training)
qplot(skewness_pitch_belt,skeweness_roll_belt,colour=classe,data=pmltraining)
qplot(skewness_pitch_belt,skewness_roll_belt,colour=classe,data=pmltraining)
qplot(log(skewness_pitch_belt),log(skewness_roll_belt),colour=classe,data=pmltraining)
qplot(avg_pitch_belt,avg_roll_belt,colour=classe,data=pmltraining)
kMeans1 <- kmeans(subset(training,select=-c(classe)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(accel_belt_x,accel_belt_y,colour=classe,data=training)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit <- train(classe~ .,data=training,method="rf",prox=TRUE)
