---
title: 'Machine Learning: Prediction of classe using Weight lifting excersises(Project)'
author: "Ambika J, as on 25-Oct-2015"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
    theme: united
    highlight: tango
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
---

<style media="screen" type="text/css">

.layer1 {
    margin-left: auto;
    margin-right: auto;
    width: 95%;
    background-color: #b0e0e6;
}

.heading {
    margin: 1px;
    color: #000;
    padding: 3px 10px;
    cursor: pointer; cursor: hand;
    position: relative;
    background-color:#ffffcc;
}
.content {
    background-color:#ffffcc;
}

p { padding: 5px 0; }
</style>

```{r function, echo=FALSE, results='hide', message=FALSE}
##=========================================##
## load_package/s:
## get the list of packages.
## parameter: packages
##
## For each package:
##         checks if package exists
##   if it doesnt, then, installs that package.
##   parameter: package_name
##=========================================##
load_packages <- function(pkgs) {
    for (i in 1:length(pkgs)) load_package(pkgs[i])
}

load_package <- function(pkg) {
	if (!isPackageInstalled(pkg)) install.packages(pkg,repos = "http://cran.us.r-project.org")
	require(pkg, character.only = TRUE)
}

if (!is.element("R.utils", installed.packages())) install.packages("R.utils",repos = "http://cran.us.r-project.org") 
require("R.utils")

load_packages(c("caret","randomForest","rpart","rpart.plot","glmnet","klaR","stargazer","e1071","mlbench","doParallel","rattle","datasets","UsingR","plyr","gridExtra","knitr"))
```
```{r pkgs, echo=FALSE, results='hide', eval=FALSE}
## List of packages required
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(glmnet)
library(klaR)
library(stargazer)
library(e1071)
library(mlbench)
#library(doParallel)
library(rattle)
library(datasets)
library(UsingR)
library(plyr)
library(gridExtra)
library(knitr)

# Hlavac, Marek (2015). stargazer: Well-Formatted Regression and Summary Statistics Tables.
# R package version 5.2. http://CRAN.R-project.org/package=stargazer
```

```{r, echo=FALSE}
rm(list=ls()) ## clear the working space
load("~/gitdir/coursera/studentcoursera/08_machine_learning/project/mach-pred.RData")
```  

### Executive Summary
**The project guidelines:**  
<UL>
<li> the requirement is  to build a machine learning algorithm to predict activity quality from activity monitors.  
<li> the goal is to use to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.  
<li> data provides how these participants performed barbell lifts correctly and incorrectly in 5 different ways. Classe: A, B, C, D, E.  
</UL>

_As part of building a machine learning prediction algorithm_,  
<UL>
<li> Chose only classe as a factor variable and raw measurement variables which are either int or num variables. Also, variables related to accelerometer.
<li> Peformed EDA to know the type of data (pattern), and it seems to be more like clusters and also a classification problem, thus, we cannot perform linear models on this. Experimented with lvq, k-means and Parallel Random forest but not stated in the project, for the sake of time and space. Other models experimented with lda, qda, glmnet and random forest. Finally, chose random forest.  
<li> Before we train, we have preprocessed data, so that the models would perform well. Like, remove NAs, zero variables and high corelated variables, whereever need be. Also removed a few rows with outliers. [read more on readme.md]  
<li> To predict, we have trained the training data using random forest with cross-validated 10 folds, tested using validation data. Several experiments were conducted and finally concluded on random forest. [read more on readme.md] 
<li> Conducted cross validations on trained data. The different error rates, with different models, are reflected.
<li> We predict the classe that it belongs to. Predict by passing through the pml-testing data, using the model used to train.    
</UL>

### Variable selection
The only factor variable is classe, others are all predictor variables which are continuous variables. We are using only raw measurements for building this algorithm. [For code and details, refer to Appendix C.]

### EDA
To understand the predictor variables, the data patterns, exploratory data analysis was conducted. 

The summary of data is provided in Appedix D, section data.

From the plot of accel scatterplot matrix [Appendix D. section "plot"], it is evident that the data is not linear, thus, we cannot process linear models for machine learning. Similar to accel scatter plot, there are scatter plots for gyros and magnet (which was peformed outside this project).
[For code and details, refer to Appendix D.]

One interesting patter noticed during EDA, refer to Appendix L, if intereseted.

### Data partition
As we have a separate testing data set, splitting this data into only training and validation. The final testing will be performed on the pmltesting data for prediction. [For code and details, refer to Appendix E.]

### Preprocess data
To improve the performance of the algorithm, we need to fine tune predictor variables. We have eliminated the irrelevant fields and rows like NAs and zero variables, highly correlated variables and also outliers. [For code and details, refer to Appendix F2.]

From Gyros and Magnet scatter plots, it is evident that there are some outliers. For variables, gyros\_dumbbell/forearm x/y/z, magnet\_dumbbell\_y. [For code and details, refer to Appendix F1.]

### Models: Experiments (training data)
For all the different experiments conducted - refer to Appendix G.
Experiments conducted on models of randomForest, Trees, lda, qda, nb, glmnet and randomforest with cross-validation.

### Final model selection to train data
After all these experiments with different models, Random forest is the chosen model, due it robustness and accuracy. And in the Random forests, Random forest with 10-fold cross validation is the with the least OOB error rate. Thus, shortlisting this one. We will go ahead with this model for our prediction. The model is modelFit_rf2. [For code, graphs and details, refer to Appendix H.]

### Cross validations
From the plot of Appendix I., it is evident that the data predicted with validation is accurate. Every class predicted is exactly the classe expected. Thus, proves that this model is the right model for this dataset.
[For code, graphs and details, refer to Appendix I.]

### Out of sample error
In the final model chosen, random forest, OOB estimate of error rate is 1.8%. This is the in sample error (resubstitution error). Whereas, With the other random forest model, we see that it was above 6%. Also, note: In-sample-error < out-of-sample-error. 
Out of sample error here, is 0, as it is 100% accuracy in this case.

Also, refer to the graph in Appendix J, We have got 100% accuracy in predicting the validation data set, in this particular experiment and dataset.
[For code, graphs and details, refer to Appendix J.]
  
### Predict testing data
Data is predicted on testing data and the results are published in Appendix K. It was 100% accurate. Thus, this model severed right for this particular case.

### Appendix
#### A: Loading data
```{r filedownload}
## Loading the data
setwd("~/gitdir/coursera/studentcoursera/08_machine_learning/project")
if (!file.exists("data"))  dir.create("data")

## File download and unzip
if(!file.exists("data/pml_training.rds")) {
    if(!file.exists("data/pml-training.csv")) 
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
            destfile = "./data/pml-training.csv", method = "curl")
}

if(!file.exists("data/pml_testing.rds")) {
    if(!file.exists("data/pml-testing.csv")) 
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
            destfile = "./data/pml-testing.csv", method = "curl")
}

```

#### B. Preprocessing the basic data
```{r loadData}
if (!"pmltraining" %in% ls()) 
    pmltraining <- read.csv("data/pml-training.csv",na.strings=c("")) 
 
if (!"pmltesting" %in% ls()) 
    pmltesting  <- read.csv("data/pml-testing.csv",na.strings=c("")) 

## Data cleaning
## assuming that this is misspelt for "skewness_pitch_belt" instead of "skewness_roll_belt.1"
names(pmltraining)[names(pmltraining) == "skewness_roll_belt.1"] <- "skewness_pitch_belt"
names(pmltraining)[names(pmltraining) == "var_total_accel_belt"] <- "var_accel_belt"
names(pmltraining) <- sub("_picth_","_pitch_",names(pmltraining))

names(pmltesting)[names(pmltesting) == "skewness_roll_belt.1"] <- "skewness_pitch_belt"
names(pmltesting)[names(pmltesting) == "var_total_accel_belt"] <- "var_accel_belt"
names(pmltesting) <- sub("_picth_","_pitch_",names(pmltesting))
```

#### C. Extract the required fields
```{r reqFields}
## Eliminates all the measurement variables and other variables, like user_name, num_window, new_window and timestamp related, as we are going to predict based on classe, this is the only factor variable, the rest are all numeric values. And consider x/y/z variables and accel related variables. [do not take total_accel and var_accel, as they are derived].
## New set.
taccel1 <- pmltraining[,c(grep("^accel|_x$|_y$|_z$",names(pmltraining)),length(pmltraining))] ## 37 variables
```

#### D. Explorartory Data Analysis
##### data
```{r dataExploratory,  cache=TRUE, results='asis'}
classAgg <- ddply(taccel1, .(classe), summarise, Max=max(accel_belt_x), Min=min(accel_belt_x),
      Mean=mean(accel_belt_x), Var=var(accel_belt_x), Std.Dev=sd(accel_belt_x))

stargazer(classAgg, type = "html", header = TRUE, out = "table1.html",
        title = "Table 1 Summary by group (classe)", notes = " ",
        rownames = TRUE, digits = 1, style = "all", summary = FALSE)

taccel <- taccel1[,c(grep("accel",names(taccel1)),length(taccel1))]

stargazer(taccel, type = "html", header = TRUE,
        title = "Table 2 Summary of all the variables", notes = " ",
        summary = TRUE, omit.summary.stat = "n", out = "table2.html",
        rownames = TRUE, digits = 1, style = "all")
```

```{r echo=FALSE}
## remove unwanted tmp variables
rm(list = c("classAgg"))
```

##### Plot
```{r EDA_plot, fig.show='asis'}
### Plots to check how the variables pattern are
## plots - accel
if(!file.exists("accel_scatter_plot.png")) {
    png("accel_scatter_plot.png", width = 1200, height = 820, units = "px");
    pairs( ~ ., data = taccel, main = "Accel Scatterplot Matrix", col = taccel$classe)
    dev.off()
}
## this has about 14 plots, so takes time to plot
```
![](accel_scatter_plot.png)

#### E. Data partition: Training and validation data
```{r partDataTrainTest}
### Creating training and validation data set: 60% and 40%
set.seed(3433)
trainIndex <- createDataPartition(taccel1$classe,p=0.6)[[1]]
training = taccel1[trainIndex,]
validation = taccel1[-trainIndex,]
```

```{r echo=FALSE}
rm("trainIndex")
```

#### F1. Removal of irrelevant data from training, after EDA
```{r echo=TRUE, eval=FALSE}
n = ncol(training)
#n = 3
for (i in 1:n) {
    t <- training[[i]]
    plot(t, main = names(training[i]))
    print(names(training[i])) 
    t1 <- sort(unique(t))
    print(rbind(head(t1, n=6),
    tail(t1, n=6)))
}
```
```{r eval=FALSE}
plot(training$gyros_forearm_y, main = "Plot for gyros_forearm_y", ylab = "gyros_forearm_y")

## From the above code, the plots were verified and these are the variables with outliers and these rows will be removed.
remove_rows <- with(training, accel_forearm_y > 592 | accel_forearm_y < -456 | gyros_forearm_y > 7 | gyros_forearm_x < -3 | magnet_dumbbell_y < -742 | accel_dumbbell_z < -273 | accel_dumbbell_x < -237 | gyros_dumbbell_z > 2 | gyros_dumbbell_y > 3 | accel_belt_y < -54 | accel_belt_y > 92 | gyros_belt_y > 0.56 | gyros_belt_y < -0.45)

training <- training[-remove_rows,]
```

#### F2. Preprocessing
```{r preProcessing}
### Eliminating - NA and zero fields; both from training and validation data
### Variability check
training_nzv <- nearZeroVar(training, saveMetrics=T)
training <- training[,!training_nzv$nzv & !training_nzv$zeroVar]
validation  <- validation [,!training_nzv$nzv & !training_nzv$zeroVar]

### eliminated fields
sum(training_nzv$nzv | training_nzv$zeroVar) ##none

### Correlation of the fields: We are not performing this.
#ncol(training)
trainCorr1 <- cor(training[,-length(training)])
highCorr1 <- findCorrelation(trainCorr1, 0.90)
names(training[,highCorr1])

training <- training[, -highCorr1]
validation  <- validation [, -highCorr1]
ncol(training) ## 33 variables
```

```{r echo=FALSE}
rm(list = grep("Corr1", ls(), value = T))
rm("training_nzv")
```

#### G. Experiment with different models
##### ML: Train data: RandomForest
```{r ML_RF_trainData, cache=TRUE, eval=FALSE}
## Train data with Random forest
if (!"modelFitRF" %in% ls()) 
    modelFitRF <- train(classe ~ ., data=training, method="rf", ntree=100)
finModRF <- modelFitRF$finalModel
plot(finModRF)
```

##### ML: Train data: Trees
```{r ML_tree_trainData, cache=TRUE}
## Train data with Trees
if (!"modelFitTree" %in% ls()) 
    modelFitTree <- train(classe ~ ., data=training, method="rpart")
finModTree <- modelFitTree$finalModel
fancyRpartPlot(finModTree)
```

##### ML: Train data: Combine - lda, qda with nb
```{r ML_lda_trainData, cache=TRUE, warning=FALSE}
## Train data with lda, qda with nb
if (!"modelFit_lda" %in% ls()) 
    modelFit_lda <- train(classe ~ ., data=training, method="lda")
if (!"modelFit_qda" %in% ls()) 
    modelFit_qda <- train(classe ~ ., data=training, method="qda")
if (!"modelFit_nb" %in% ls()) 
    modelFit_nb <- train(classe ~ ., data=training, method="nb")

pred_lda <- predict(modelFit_lda, validation)
pred_qda <- predict(modelFit_qda, validation)
pred_nb  <- predict( modelFit_nb, validation)

table(pred_lda,pred_nb)
table(pred_qda,pred_nb)
```

**Comparison of Results**  
```{r, eval=FALSE}
pred_posb_l <- (pred_lda==pred_nb)
pred_posb_q <- (pred_qda==pred_nb)

p1 <- qplot(accel_arm_x,accel_arm_y,data=validation,colour=pred_posb_l)
p2 <- qplot(accel_arm_x,accel_arm_y,data=validation,colour=pred_posb_q)
grid.arrange(p1, p2, ncol=2)
```

From these comparisons it is evident that False rate is on to the higher side for both lda and qda models when compared to mobel nb.

```{r echo=FALSE}
rm(list = grep("pred_", ls(), value = T))
```

##### ML: Train data: combine: glmnet and rf
```{r ML_glm_trainData, cache=TRUE}

## Train control
trCntrl <- trainControl(classProbs=TRUE, method="cv", number=10, allowParallel=TRUE, returnData=FALSE)

## Train data with glmnet and rf
if (!"modelFit_glm1" %in% ls()) 
    modelFit_glm1 <- train(classe ~ ., data=training, method="glmnet", trControl=trCntrl,
                          metric="Accuracy", maximize=TRUE)

if (!"modelFit_rf2" %in% ls()) 
    modelFit_rf2 <- train(classe ~ ., data=training, method="rf", trControl=trCntrl,
                          metric="Accuracy", maximize=TRUE)

pred_glm <- predict(modelFit_glm1, validation)
pred_rf2 <- predict(modelFit_rf2, validation)
```

```{r, fig.height=4, fig.width=4, eval=FALSE}
qplot(pred_glm,pred_rf2,data=validation,colour=classe)
```  

From this plot it is evident, that the combination of these are not appropriate. As the prediction from glmnet and rf2, do not tally. 

```{r}
table(pred_glm, pred_rf2)
```

```{r echo=FALSE}
#rm(list = grep("pred_", ls(), value = T))
```

#### H. Final Model selection
```{r}
finMod_rf2 <- modelFit_rf2$finalModel
plot(finMod_rf2)
```

```{r, fig.height=7, fig.width=5, eval=FALSE}
varImpPlot(finMod_rf2)
```

```{r echo=FALSE}
rm("finMod_rf2")
```

#### I. Cross Validation
```{r}
pred_t <- predict(modelFit_rf2, validation)
```

```{r, fig.height=4, fig.width=4}
qplot(classe,pred_t,colour=classe,data=validation)
```

From this plot it is evident that the data predicted with validation is accurate. Every class predicted is exactly the classe expected. Thus, proves that this model is the right model for this dataset.

```{r}
confusionMatrix(validation$classe,pred_t)
```

```{r echo=FALSE}
t <- rbind(
    predict=table(pred_t),
    validation=table(validation$classe)
    )
dif_per <- round((t[1,] - t[2,])/t[1,]*100,2)
accurcy <- 100 - abs(dif_per)
rbind(t,dif_per,accurcy)
```

```{r echo=FALSE}
rm(list = c("t","dif_per","accurcy"))
```

#### J. Out of sample error
In the final model chosen, random forest, OOB estimate of  error rate is 1.8%. This is the in sample error (resubstitution error). Whereas, With the other random forest model, we see that it was above 6%. Also, note: In-sample-error < out-of-sample-error

Out of sample error(generalization error):
```{r out-sample-error}
table(pred_rf2, pred_t)

pred_posb_val <- (pred_rf2==pred_t)

## accuracy
mean(pred_posb_val)

## out sample error
1 - mean(pred_posb_val)
```

```{r, eval=FALSE}
qplot(accel_arm_x, accel_arm_y, data=validation, colour=pred_posb_val)
```

#### K. Predict on testing set and file creation
```{r testDataPred, echo=FALSE}
pred <- predict(modelFit_rf2, pmltesting)
pred
table(pred)

#create a separate directory
setwd("~/gitdir/coursera/studentcoursera/08_machine_learning/project")
if (!file.exists("test_output"))  dir.create("test_output")

# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("test_output/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pred)
```

```{r echo=FALSE}
rm(list = c("pred","pml_write_files"))
```

```{r saveRdata, echo=FALSE}
## save the entire data as is
save.image("~/gitdir/coursera/studentcoursera/08_machine_learning/project/mach-pred.RData")
```

#### L. One interesting pattern noticed (dumbbell), during EDA.
```{r cache=TRUE}
newTrain1 <- subset(pmltraining,select=c(grep("^total_|_x$|_y$|_z$|^roll_|^pitch_|^yaw",names(pmltraining)),grep("classe",names(pmltraining))))

pairs(classe~roll_dumbbell+yaw_dumbbell+pitch_dumbbell,data=newTrain1, 
      main="dumbbell Scatterplot Matrix",col=newTrain1$classe)
```

```{r echo=FALSE}
rm(list = grep("newTrain1", ls(), value = T))
```

#### M. Session Info
```{r}
sessionInfo()
```

### References
  
Qualitative Activity Recognition of Weight Lifting Exercises  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th Augmented Human (AH) International Conference in cooperation with ACM SIGCHI (Augmented Human'13) . Stuttgart, Germany: ACM SIGCHI, 2013.  
Read more: [Weight Lifting Exercise](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201#ixzz3oNTQE11n)  

Link: [2013.Velloso.QAR-WLE.pdf](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf)